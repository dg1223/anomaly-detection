import csv
import pandas as pd
import pyspark.sql.functions as f
import pyspark.sql.types
from pyspark.sql.types import (
    StructType,
    StructField,
    StringType,
    ArrayType,
    DoubleType,
    TimestampType,
    LongType,
    IntegerType,
)
from pyspark.sql.session import SparkSession
import os
import json
from IPython.display import display

spark = SparkSession.builder.getOrCreate()
from src.caaswx.spark._transformers.sessionfeaturegenerator import (
    SessionFeatureGenerator,
)


# df = spark.createDataFrame(pd.read_csv('data/export(3).csv'))
# df = df.withColumn(
#             "SM_TIMESTAMP",
#             f.col("SM_TIMESTAMP").cast(
#                 TimestampType()
#             ),
#         )
# df.write.parquet('data/parquet_data/ip_feature_generator_tests/df.parquet')
# test_df = spark.read.parquet('data/parquet_data/ip_feature_generator_tests/df.parquet')
#
# test_df.show()

ans_schema = StructType(
    [
        StructField("SM_SESSIONID", StringType()),
        StructField(
            "window",
            StructType(
                [StructField("start", StringType()), StructField("end", StringType()),]
            ),
            False,
        ),
        StructField("SESSION_APP", ArrayType(StringType())),
        StructField("COUNT_UNIQUE_APPS", LongType()),
        StructField("SESSION_USER", ArrayType(StringType())),
        StructField("COUNT_ADMIN_LOGIN", LongType()),
        StructField("COUNT_ADMIN_LOGOUT", LongType()),
        StructField("COUNT_AUTH_ATTEMPT", LongType()),
        StructField("COUNT_AUTH_REJECT", LongType()),
        StructField("COUNT_FAILED", LongType()),
        StructField("COUNT_VISIT", LongType()),
        StructField("COUNT_GET", LongType()),
        StructField("COUNT_POST", LongType()),
        StructField("COUNT_HTTP_METHODS", LongType()),
        StructField("COUNT_RECORDS", LongType()),
        StructField("COUNT_UNIQUE_ACTIONS", LongType()),
        StructField("COUNT_UNIQUE_EVENTS", LongType()),
        StructField("COUNT_UNIQUE_IPS", LongType()),
        StructField("COUNT_UNIQUE_RESOURCES", LongType()),
        StructField("COUNT_UNIQUE_REP", LongType()),
        StructField("SESSION_SM_ACTIONS", ArrayType(StringType())),
        StructField("SESSION_SM_RESOURCE", ArrayType(StringType())),
        StructField("SESSION_REP_APP", ArrayType(StringType())),
        StructField("SESSSION_FIRST_TIME_SEEN", StringType()),
        StructField("SESSSION_LAST_TIME_SEEN", StringType()),
        StructField("SDV_BT_RECORDS", DoubleType()),
    ]
)

ans_data = [
    (
        "+8k93/7mL4V5xFpdrYof8DbQO3U=",
        {
            "start": "2018-11-09T20:45:00.000+0000",
            "end": "2018-11-09T21:00:00.000+0000",
        },
        ["/gol-ged/"],
        1,
        ["cn=f41b05cc-777d-4259-ab8d-96baa58b1e18"],
        0,
        0,
        0,
        0,
        0,
        0,
        7,
        0,
        7,
        7,
        1,
        1,
        1,
        7,
        2,
        ["GET"],
        [
            "/gol-ged/iica/repdtc/prot/Xfer?xTo=taxReturnsTabId",
            "/gol-ged/mima/repmyccnt/prot/txrtrns.action",
            "/gol-ged/mima/repmyccnt/prot/unrdMlCnt.action",
            "/gol-ged/tuvr/repvmr/prot/ajaxVwRtnLst.action",
            "/gol-ged/iica/repdtc/prot/viewDTCInfoAjax.action",
            "/gol-ged/iica/repdtc/prot/strtVwDTCInfAjax.action",
            "/gol-ged/tuvr/repvmr/prot/ajaxCrryvr.action",
        ],
        ["repdtc/", "repmyccnt/", "repvmr/"],
        "2018-11-09T20:51:50.000+0000",
        "2018-11-09T20:51:51.000+0000",
        0.377964473009227,
    )
]

ans_df = spark.createDataFrame(ans_data, schema=ans_schema)

ans_df = ans_df.withColumn(
    "window",
    f.col("window").cast(
        StructType(
            [
                StructField("start", TimestampType()),
                StructField("end", TimestampType()),
            ]
        )
    ),
)
ans_df = ans_df.withColumn(
    "SESSSION_FIRST_TIME_SEEN", f.col("SESSSION_FIRST_TIME_SEEN").cast(TimestampType()),
)
ans_df = ans_df.withColumn(
    "SESSSION_LAST_TIME_SEEN", f.col("SESSSION_LAST_TIME_SEEN").cast(TimestampType()),
)
ans_df.write.parquet(
    "./data/parquet_data/session_feature_generator_tests/ans_data.parquet"
)

test_df = spark.read.parquet(
    "data/parquet_data/session_feature_generator_tests/df.parquet"
)
ans_1_data = spark.read.parquet(
    "./data/parquet_data/session_feature_generator_tests/ans_data.parquet"
)
fg = SessionFeatureGenerator()
result = fg.transform(test_df)
ans_1_data.show()
# # ans_1_data.printSchema()
result.show()
subtractresult = result.union(ans_1_data).subtract(result.intersect(ans_1_data)).toPandas()
# result.subtract(ans_1_data).show()
print(result.subtract(ans_1_data).count() == 0)
